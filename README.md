
<p align="center" width="100%">
<a target="_blank"><img src="figs/video_llama_logo.jpg" alt="Video-LLaMA" style="width: 50%; min-width: 200px; display: block; margin: auto;"></a>
</p>

# Video-LLaMA: Video Anlamada EÄŸitimli GÃ¶rsel-Ä°ÅŸitsel Dil Modeli
<!-- **Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding** -->

Bu, bÃ¼yÃ¼k dil modellerini video ve ses anlama yetenekleriyle gÃ¼Ã§lendirmeyi amaÃ§layan Video-LLaMA projesinin deposudur.

<div style='display:flex; gap: 0.25rem; '>
<a href='https://modelscope.cn/studios/damo/video-llama/summary'><img src='https://img.shields.io/badge/ModelScope-Demo-blueviolet'></a>
<a href='https://www.modelscope.cn/models/damo/videollama_7b_llama2_finetuned/summary'><img src='https://img.shields.io/badge/ModelScope-Checkpoint-blueviolet'></a>
<a href='https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
<a href='https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Checkpoint-blue'></a> 
<a href='https://arxiv.org/abs/2306.02858'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
</div>

## Haberler
- <h3> [2024.06.03] ğŸš€ğŸš€ Daha gÃ¼Ã§lÃ¼ performans ve kullanÄ±mÄ± daha kolay kod tabanÄ± ile <a href='https://github.com/DAMO-NLP-SG/VideoLLaMA2'>VideoLLaMA2</a>'yi resmi olarak yayÄ±nlÄ±yoruz, deneyin!</h3>
- [11.14] â­ï¸ Mevcut README dosyasÄ± yalnÄ±zca **Video-LLaMA-2** (dil Ã§Ã¶zÃ¼cÃ¼ olarak LLaMA-2-Chat) iÃ§indir, Ã¶nceki Video-LLaMA sÃ¼rÃ¼mÃ¼nÃ¼ (dil Ã§Ã¶zÃ¼cÃ¼ olarak Vicuna) kullanma talimatlarÄ±na [buradan](https://github.com/DAMO-NLP-SG/Video-LLaMA/blob/main/README_Vicuna.md) ulaÅŸabilirsiniz.
- [08.03] ğŸš€ğŸš€ Dil Ã§Ã¶zÃ¼cÃ¼ olarak [Llama-2-7B/13B-Chat](https://huggingface.co/meta-llama) kullanan **Video-LLaMA-2**'yi yayÄ±nladÄ±k
    - ArtÄ±k delta aÄŸÄ±rlÄ±klarÄ± ve ayrÄ± Q-former aÄŸÄ±rlÄ±klarÄ± YOK, Video-LLaMA'yÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in gereken tÃ¼m aÄŸÄ±rlÄ±klar burada :point_right: [[7B](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned)][[13B](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned)] 
    - Ã–nceden eÄŸitilmiÅŸ kontrol noktalarÄ±mÄ±zdan baÅŸlayarak daha fazla Ã¶zelleÅŸtirmeye izin verir [[7B-Pretrained](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Pretrained)] [[13B-Pretrained](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Pretrained)]
- [06.14] **NOT**: Mevcut Ã§evrimiÃ§i interaktif demo Ã¶ncelikle Ä°ngilizce sohbet iÃ§indir ve Vicuna/LLaMA Ã‡ince metinleri Ã§ok iyi temsil edemediÄŸinden Ã‡ince sorular sormak iÃ§in **Ä°YÄ°** bir seÃ§enek olmayabilir.
- [06.13] **NOT**: Åu anda diÄŸer Ã§Ã¶zÃ¼cÃ¼ler iÃ§in birkaÃ§ VL kontrol noktamÄ±z olmasÄ±na raÄŸmen, ses desteÄŸi **YALNIZCA** Vicuna-7B iÃ§indir.
- [06.10] **NOT**: TÃ¼m Ã§erÃ§eve (ses dalÄ± ile) A10-24G'de normal olarak Ã§alÄ±ÅŸamadÄ±ÄŸÄ±ndan HF demosunu henÃ¼z gÃ¼ncellemedik. Mevcut Ã§alÄ±ÅŸan demo hala Ã¶nceki Video-LLaMA sÃ¼rÃ¼mÃ¼dÃ¼r. Bu sorunu yakÄ±nda dÃ¼zelteceÄŸiz.
- [06.08] ğŸš€ğŸš€ Ses destekli Video-LLaMA'nÄ±n kontrol noktalarÄ±nÄ± yayÄ±nladÄ±k. DokÃ¼mantasyon ve Ã¶rnek Ã§Ä±ktÄ±lar da gÃ¼ncellendi.    
- [05.22] ğŸš€ğŸš€ Ä°nteraktif demo Ã§evrimiÃ§i, Video-LLaMA'mÄ±zÄ± (dil Ã§Ã¶zÃ¼cÃ¼ olarak **Vicuna-7B** ile) [Hugging Face](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA) ve [ModelScope](https://pre.modelscope.cn/studios/damo/video-llama/summary)'da deneyin!!
- [05.22] â­ï¸ Vicuna-7B ile oluÅŸturulan **Video-LLaMA v2**'yi yayÄ±nladÄ±k
- [05.18] ğŸš€ğŸš€ Ã‡ince video tabanlÄ± sohbeti destekler 
    - [**Video-LLaMA-BiLLA**](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series/resolve/main/finetune-billa7b-zh.pth): Dil Ã§Ã¶zÃ¼cÃ¼ olarak [BiLLa-7B-SFT](https://huggingface.co/Neutralzz/BiLLa-7B-SFT)'yi tanÄ±ttÄ±k ve video-dil hizalÄ± modeli (yani, aÅŸama 1 modeli) makine Ã§evirisi yapÄ±lmÄ±ÅŸ [VideoChat](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data) talimatlarÄ± ile ince ayar yaptÄ±k.   
    - [**Video-LLaMA-Ziya**](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series/resolve/main/finetune-ziya13b-zh.pth): Video-LLaMA-BiLLA ile aynÄ± ancak dil Ã§Ã¶zÃ¼cÃ¼ [Ziya-13B](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1) olarak deÄŸiÅŸtirildi.    
- [05.18] â­ï¸ Video-LLaMA'mÄ±zÄ±n tÃ¼m varyantlarÄ±nÄ±n model aÄŸÄ±rlÄ±klarÄ±nÄ± saklamak iÃ§in bir Hugging Face [deposu](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series) oluÅŸturduk.
- [05.15] â­ï¸ [**Video-LLaMA v2**](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series/resolve/main/finetune-vicuna13b-v2.pth)'yi yayÄ±nladÄ±k: Video-LLaMA'nÄ±n talimat izleme yeteneÄŸini daha da geliÅŸtirmek iÃ§in [VideoChat](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data) tarafÄ±ndan saÄŸlanan eÄŸitim verilerini kullandÄ±k.
- [05.07] Ã–nceden eÄŸitilmiÅŸ ve talimat ayarlÄ± kontrol noktalarÄ± dahil olmak Ã¼zere **Video-LLaMA**'nÄ±n ilk sÃ¼rÃ¼mÃ¼nÃ¼ yayÄ±nladÄ±k.

<p align="center" width="100%">
<a target="_blank"><img src="figs/architecture_v2.png" alt="Video-LLaMA" style="width: 80%; min-width: 200px; display: block; margin: auto;"></a>
</p>

## GiriÅŸ

- Video-LLaMA, [BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) ve [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) Ã¼zerine inÅŸa edilmiÅŸtir. Ä°ki temel bileÅŸenden oluÅŸur: (1) GÃ¶rÃ¼ntÃ¼-Dil (VL) DalÄ± ve (2) Ses-Dil (AL) DalÄ±.
  - **VL DalÄ±** (GÃ¶rsel kodlayÄ±cÄ±: ViT-G/14 + BLIP-2 Q-Former)
    - Video temsillerini hesaplamak iÃ§in iki katmanlÄ± bir video Q-Former ve bir kare gÃ¶mme katmanÄ± (her karenin gÃ¶mÃ¼lmelerine uygulanan) tanÄ±tÄ±lmÄ±ÅŸtÄ±r. 
    - VL DalÄ±nÄ± Webvid-2M video baÅŸlÄ±k veri setinde video-metin Ã¼retme gÃ¶revi ile eÄŸitiyoruz. Statik gÃ¶rsel kavramlarÄ±n anlaÅŸÄ±lmasÄ±nÄ± geliÅŸtirmek iÃ§in Ã¶n eÄŸitim veri setine gÃ¶rÃ¼ntÃ¼-metin Ã§iftleri (~595K gÃ¶rÃ¼ntÃ¼ baÅŸlÄ±ÄŸÄ± [LLaVA](https://github.com/haotian-liu/LLaVA)'dan) de ekledik.
    - Ã–n eÄŸitimden sonra, VL DalÄ±mÄ±zÄ± [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLaVA](https://github.com/haotian-liu/LLaVA) ve [VideoChat](https://github.com/OpenGVLab/Ask-Anything)'ten alÄ±nan talimat ayarlama verileri kullanarak daha da ince ayar yapÄ±yoruz. 
  - **AL DalÄ±** (Ses kodlayÄ±cÄ±: ImageBind-Huge) 
    - Ses temsillerini hesaplamak iÃ§in iki katmanlÄ± bir ses Q-Former ve bir ses segment gÃ¶mme katmanÄ± (her ses segmentinin gÃ¶mÃ¼lmesine uygulanan) tanÄ±tÄ±lmÄ±ÅŸtÄ±r.
    - KullanÄ±lan ses kodlayÄ±cÄ± (yani ImageBind) zaten birden Ã§ok modalite arasÄ±nda hizalanmÄ±ÅŸ olduÄŸundan, AL DalÄ±nÄ± yalnÄ±zca ImageBind Ã§Ä±kÄ±ÅŸÄ±nÄ± dil Ã§Ã¶zÃ¼cÃ¼ye baÄŸlamak iÃ§in sadece video/gÃ¶rÃ¼ntÃ¼ talimat verileri Ã¼zerinde eÄŸitiyoruz.    
- Ã‡apraz modal eÄŸitim sÄ±rasÄ±nda yalnÄ±zca Video/Ses Q-Former, konumsal gÃ¶mme katmanlarÄ± ve doÄŸrusal katmanlar eÄŸitilebilir.

## Ã–rnek Ã‡Ä±ktÄ±lar

- **Arka plan sesli video**

<p float="left">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/7f7bddb2-5cf1-4cf4-bce3-3fa67974cbb3" style="width: 45%; margin: auto;">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/ec76be04-4aa9-4dde-bff2-0a232b8315e0" style="width: 45%; margin: auto;">
</p>

- **Ses efektsiz video**
<p float="left">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/539ea3cc-360d-4b2c-bf86-5505096df2f7" style="width: 45%; margin: auto;">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/7304ad6f-1009-46f1-aca4-7f861b636363" style="width: 45%; margin: auto;">
</p>

- **Statik gÃ¶rÃ¼ntÃ¼**
<p float="left">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/a146c169-8693-4627-96e6-f885ca22791f" style="width: 45%; margin: auto;">
    <img src="https://github.com/DAMO-NLP-SG/Video-LLaMA/assets/18526640/66fc112d-e47e-4b66-b9bc-407f8d418b17" style="width: 45%; margin: auto;">
</p>

## Ã–nceden EÄŸitilmiÅŸ & Ä°nce AyarlÄ± Kontrol NoktalarÄ±

~~AÅŸaÄŸÄ±daki kontrol noktalarÄ± yalnÄ±zca Ã¶ÄŸrenilebilir parametreleri (konumsal gÃ¶mme katmanlarÄ±, Video/Ses Q-former ve doÄŸrusal projeksiyon katmanlarÄ±) saklar.~~

AÅŸaÄŸÄ±daki kontrol noktalarÄ± Video-LLaMA'yÄ± baÅŸlatmak iÃ§in tam aÄŸÄ±rlÄ±klardÄ±r (gÃ¶rsel kodlayÄ±cÄ± + ses kodlayÄ±cÄ± + Q-Former'lar + dil Ã§Ã¶zÃ¼cÃ¼):

| Kontrol NoktasÄ±       | BaÄŸlantÄ± | Not |
|:------------------|-------------|-------------|
| Video-LLaMA-2-7B-Pretrained    | [baÄŸlantÄ±](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned/tree/main)       | WebVid (2.5M video-baÅŸlÄ±k Ã§ifti) ve LLaVA-CC3M (595k gÃ¶rÃ¼ntÃ¼-baÅŸlÄ±k Ã§ifti) Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ |
| Video-LLaMA-2-7B-Finetuned | [baÄŸlantÄ±](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/tree/main) | [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLaVA](https://github.com/haotian-liu/LLaVA) ve [VideoChat](https://github.com/OpenGVLab/Ask-Anything)'ten alÄ±nan talimat ayarlama verileri Ã¼zerinde ince ayar yapÄ±lmÄ±ÅŸ |

| Video-LLaMA-2-13B-Pretrained    | [baÄŸlantÄ±](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Pretrained/tree/main)       | 7B modeli ile aynÄ± ancak daha bÃ¼yÃ¼k dil Ã§Ã¶zÃ¼cÃ¼ (LLaMA-2-13B-Chat) |
| Video-LLaMA-2-13B-Finetuned | [baÄŸlantÄ±](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned/tree/main) | 7B modeli ile aynÄ± ancak daha bÃ¼yÃ¼k dil Ã§Ã¶zÃ¼cÃ¼ (LLaMA-2-13B-Chat) |

## Kurulum

1. Depoyu klonlayÄ±n ve baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:
```bash
git clone https://github.com/DAMO-NLP-SG/Video-LLaMA.git
cd Video-LLaMA
pip install -r requirements.txt
```

2. Gerekli modellerle ilgili detaylara [buradan](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) ulaÅŸabilirsiniz:
- LLaMA-2-7B/13B-Chat
- Ä°ndirdiÄŸiniz LLaMA-2 model dizinini `model_path`'de belirttiÄŸiniz yere yerleÅŸtirin (bkz: aÅŸaÄŸÄ±daki demo kodu). 

3. Eva ViT gÃ¶rsel kodlayÄ±cÄ±yÄ± ve ImageBind ses kodlayÄ±cÄ±yÄ± iÃ§eren [model Ã¶n eÄŸitimi kaydÄ±nÄ±](https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned/tree/main) indirin.

4. Video iÃ§inden kare Ã§Ä±karmak iÃ§in ffmpeg kurulu olmalÄ±dÄ±r, Ã¶rneÄŸin:  
```bash
# Ubuntu
apt update
apt install ffmpeg
# Windows (PowerShell Admin)
choco install ffmpeg
# MacOS
brew install ffmpeg
```

## Demo BaÅŸlatma

**Not**: Video-LLaMA'nÄ±n ses desteÄŸinin **YALNIZCA** Vicuna-7B iÃ§in olduÄŸunu ve diÄŸer dil Ã§Ã¶zÃ¼cÃ¼leri iÃ§in henÃ¼z mevcut olmadÄ±ÄŸÄ±nÄ± lÃ¼tfen unutmayÄ±n.

1. Online Demo veya Local Demo arasÄ±nda seÃ§im yapÄ±n:

### Online Demo
- [ModelScope](https://modelscope.cn/studios/damo/video-llama/summary)
- [Hugging Face](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA)

### Local Demo
**Not**: Ses desteÄŸinin **YALNIZCA** Vicuna-7B iÃ§in olduÄŸunu ve diÄŸer dil Ã§Ã¶zÃ¼cÃ¼leri iÃ§in henÃ¼z mevcut olmadÄ±ÄŸÄ±nÄ± lÃ¼tfen unutmayÄ±n.
```python
import torch
from video_llama.constants import *
from video_llama.conversation import conv_templates, SeparatorStyle
from video_llama.model.builder import load_pretrained_model
from video_llama.utils.utils import disable_torch_init
from video_llama.processor import load_processor, process_images, process_video, process_audio

# ffmpeg yÃ¼klÃ¼ olmalÄ±dÄ±r
def initialize_model(model_path):
    disable_torch_init()
    
    model_name = "video_llama"
    device = torch.device("cuda") if torch.cuda.is_available() else "cpu"
    
    model = load_pretrained_model(
        model_path=model_path,
        model_base=None,
        model_name=model_name,
        device=device,
        half=True,
        verbose=True
    )
    
    vis_processor = load_processor()
    return model, vis_processor, device

# Modeli yÃ¼kle
model_path = "DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned" # veya "DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned"
model, vis_processor, device = initialize_model(model_path)

# KonuÅŸmayÄ± baÅŸlat
conv = conv_templates["v1"].copy()
conv.messages = []

# Herhangi bir video/gÃ¶rÃ¼ntÃ¼/ses girdisi ile etkileÅŸime geÃ§
video_path = "val_video/1.mp4"
prompt = "Bu videoda ne oluyor?"

if video_path.endswith(('.mp4', '.avi', '.mov')):  # Video dosyasÄ±
    imgs, audio = process_video(video_path, vis_processor, device)
    audio_flag = True if audio is not None else False
    if len(imgs) > 0:  # Video kareleri baÅŸarÄ±yla Ã§Ä±karÄ±ldÄ±ysa
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], "Anlayabilmem iÃ§in biraz dÃ¼ÅŸÃ¼nmeme izin verin...")
        output = model.generate(imgs, audio if audio_flag else None, conv, temperature=0.7 if audio_flag else 0.2)
        conv.messages[-1][-1] = output
        print(f"Asistan: {output}")

elif video_path.endswith(('.jpg', '.png')):  # GÃ¶rÃ¼ntÃ¼ dosyasÄ±
    image = process_images(video_path, vis_processor, device)
    if image is not None:  # GÃ¶rÃ¼ntÃ¼ baÅŸarÄ±yla yÃ¼klendiyse
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], "Anlayabilmem iÃ§in biraz dÃ¼ÅŸÃ¼nmeme izin verin...")
        output = model.generate(image, None, conv, temperature=0.2)
        conv.messages[-1][-1] = output
        print(f"Asistan: {output}")

else:
    print("Desteklenmeyen dosya formatÄ±!")
```

## SÄ±nÄ±rlamalar ve Sorumluluk Reddi

* Video-LLaMA zaman zaman halÃ¼sinasyonlar yaÅŸayabilir, bu nedenle sonuÃ§larÄ±n doÄŸruluÄŸundan emin olunmasÄ± Ã¶nerilir.
* Video-LLaMA tehlikeli, yasa dÄ±ÅŸÄ±, aÃ§Ä±k saÃ§Ä±k, Ã¶nyargÄ±lÄ± ya da baÅŸka ÅŸekilde uygunsuz iÃ§erik Ã¼retmemeye Ã§alÄ±ÅŸsa da beklenmedik sonuÃ§lar Ã¼retebilir.
* Video-LLaMA'nÄ±n Ã§Ä±ktÄ±larÄ± yalnÄ±zca araÅŸtÄ±rma amaÃ§lÄ± kullanÄ±lmalÄ±dÄ±r.
* Ä°lk Ä±sÄ±nma Ã§Ä±ktÄ±sÄ±nÄ±n kalitesi genellikle dÃ¼ÅŸÃ¼k olabilir.

## AtÄ±fta Bulunma

```bibtex
@article{zhang2023videollama,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}
```

## Lisans
- Video-LLaMA'nÄ±n kaynak kodu [Apache 2.0](LICENSE) lisansÄ± altÄ±nda yayÄ±nlanmÄ±ÅŸtÄ±r.
- KullandÄ±ÄŸÄ±mÄ±z modeller ve veri setleri iÃ§in lÃ¼tfen ilgili lisanslara baÅŸvurun: [LLaMA](https://github.com/facebookresearch/llama), [ImageBind](https://github.com/facebookresearch/ImageBind), [Eva-CLIP](https://github.com/baaivision/EVA/tree/master/EVA-CLIP).

## TeÅŸekkÃ¼rler
- [BLIP-2](https://huggingface.co/docs/transformers/main/model_doc/blip-2) video-dil Ã¶n eÄŸitimi iÃ§in temel almÄ±ÅŸ olduÄŸumuz gÃ¶rÃ¼ntÃ¼-dil modelidir.
- [ImageBind](https://github.com/facebookresearch/ImageBind) ses modalitesini diÄŸer modalitelerle hizalamak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
- [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) temel etkileÅŸim kodu ve demo UI iÃ§in temel oluÅŸturmuÅŸtur.
- [LLaMA](https://github.com/facebookresearch/llama) ve [Vicuna](https://github.com/lm-sys/FastChat) dil Ã§Ã¶zÃ¼cÃ¼lerimiz iÃ§in temel modellerdir.
- [ğŸ¤— Hugging Face](https://github.com/huggingface) tÃ¼m modeller ve uygulamalarÄ± barÄ±ndÄ±rdÄ±ÄŸÄ± iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.